{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb9c9a5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-12T04:02:11.356956Z",
     "iopub.status.busy": "2022-02-12T04:02:11.355666Z",
     "iopub.status.idle": "2022-02-12T04:02:13.672173Z",
     "shell.execute_reply": "2022-02-12T04:02:13.671242Z",
     "shell.execute_reply.started": "2022-02-12T01:38:52.374700Z"
    },
    "papermill": {
     "duration": 2.328993,
     "end_time": "2022-02-12T04:02:13.672378",
     "exception": false,
     "start_time": "2022-02-12T04:02:11.343385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "%matplotlib inline\n",
    "\\\n",
    "dtype = torch.float32\n",
    "\n",
    "# Random state seed\n",
    "seed = 1234\n",
    "# For reproducibility\n",
    "torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T04:02:13.727330Z",
     "iopub.status.busy": "2022-02-12T04:02:13.715100Z",
     "iopub.status.idle": "2022-02-12T04:02:13.896363Z",
     "shell.execute_reply": "2022-02-12T04:02:13.895746Z",
     "shell.execute_reply.started": "2022-02-12T01:38:52.391299Z"
    },
    "papermill": {
     "duration": 0.216035,
     "end_time": "2022-02-12T04:02:13.896548",
     "exception": false,
     "start_time": "2022-02-12T04:02:13.680513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('/kaggle/input/happy-dataset/train_happy.h5', \"r\")\n",
    "    test_dataset = h5py.File('/kaggle/input/happy-dataset/test_happy.h5', \"r\")\n",
    "\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "    train_set_y_orig = np.transpose(train_set_y_orig.reshape((1, train_set_y_orig.shape[0])))\n",
    "    test_set_y_orig = np.transpose(test_set_y_orig.reshape((1, test_set_y_orig.shape[0])))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "train_set_X, train_set_y, test_set_X, test_set_y, classes = load_dataset()\n",
    "# print(\"train set size\", train_set_X.shape)\n",
    "# print(\"train label size\", train_set_y.shape)\n",
    "# print(\"test set size\", test_set_X.shape)\n",
    "# print(\"test label size\", test_set_y.shape)\n",
    "# print(\"classes size\", classes.shape)\n",
    "# print(classes)\n",
    "# print(train_set_y[123])\n",
    "# plt.imshow(train_set_X[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793eb34e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T04:02:13.916372Z",
     "iopub.status.busy": "2022-02-12T04:02:13.915327Z",
     "iopub.status.idle": "2022-02-12T04:02:14.069565Z",
     "shell.execute_reply": "2022-02-12T04:02:14.070095Z",
     "shell.execute_reply.started": "2022-02-12T01:38:52.698436Z"
    },
    "papermill": {
     "duration": 0.166125,
     "end_time": "2022-02-12T04:02:14.070270",
     "exception": false,
     "start_time": "2022-02-12T04:02:13.904145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (480, 64, 64, 3)\n",
      "Validation set size: (120, 64, 64, 3)\n",
      "Train set size: (480, 64, 64, 3)\n",
      "Train label size: (480, 1)\n",
      "Validation set size: (120, 64, 64, 3)\n",
      "Validation set size: (120, 1)\n",
      "Training data shape (480, 12288)\n",
      "Validation data shape (120, 12288)\n",
      "Test data shape (150, 12288)\n",
      "142.07640957302516\n",
      "81.1476808724206\n"
     ]
    }
   ],
   "source": [
    "def create_validation_set(train_set_X, train_set_y, test_size=0.2, random_state=seed):\n",
    "    \"\"\"\n",
    "    Divides the training set into training and validation set\n",
    "    \n",
    "    Input:\n",
    "    - train_set_X: Training set samples containing only features (no label)\n",
    "    - train_set_y: Training set labels\n",
    "    - test_size: (optional) % of training data to be separated as validation data\n",
    "    \n",
    "    Output:\n",
    "    - X_train: Training samples\n",
    "    - y_train: Training labels\n",
    "    - X_valid: Validation samples\n",
    "    - y_valid: Validation labels\n",
    "    \"\"\"\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_set_X, train_set_y, test_size=test_size, random_state=seed)\n",
    "    print(\"Train set size:\", X_train.shape)\n",
    "    print(\"Validation set size:\", X_valid.shape)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "train_X, y_train, valid_X, y_valid = create_validation_set(train_set_X, train_set_y, test_size=0.2)\n",
    "print(\"Train set size:\", train_X.shape)\n",
    "print(\"Train label size:\", y_train.shape)\n",
    "print(\"Validation set size:\", valid_X.shape)\n",
    "print(\"Validation set size:\", y_valid.shape)\n",
    "\n",
    "# Preparing the data\n",
    "X_train = train_X.reshape(480, 64*64*3)\n",
    "X_valid = valid_X.reshape(120, 64*64*3)\n",
    "test_set_X = test_set_X.reshape(150, 64*64*3)\n",
    "print(\"Training data shape\", X_train.shape)\n",
    "print(\"Validation data shape\", X_valid.shape)\n",
    "print(\"Test data shape\", test_set_X.shape)\n",
    "\n",
    "def normalization(x, mu, std):\n",
    "    \"\"\"\n",
    "    Normalization\n",
    "    \n",
    "    Input:\n",
    "    - x: data\n",
    "    - mu: average\n",
    "    - std: standard deviation\n",
    "    \n",
    "    Output:\n",
    "    - x_scaled: normalized output\n",
    "    \"\"\"\n",
    "    x_scaled = (x-mu)/std\n",
    "    return x_scaled\n",
    "\n",
    "# Normalize the data\n",
    "mean_X = X_train.mean()\n",
    "print(mean_X)\n",
    "std_X = X_train.std()\n",
    "print(std_X)\n",
    "X_train_scl = normalization(X_train, mean_X, std_X)\n",
    "X_valid_scl = normalization(X_valid, mean_X, std_X)\n",
    "X_test_scl = normalization(test_set_X, mean_X, std_X)\n",
    "# print(\"Train size\", X_train_scl.shape)\n",
    "# print(\"Maximum of ttrain\", X_train_scl.max())\n",
    "# print(\"Mean of ttrain\", X_train_scl.mean())\n",
    "# print(\"Validation size\", X_valid_scl.shape)\n",
    "# print(\"Maximum of valid\", X_valid_scl.max())\n",
    "# print(\"Mean of valid\", X_valid_scl.mean())\n",
    "# print(\"Test size\", X_test_scl.shape)\n",
    "# print(\"Maximum of test\", X_test_scl.max())\n",
    "# print(\"Mean of test\", X_test_scl.mean())\n",
    "\n",
    "##################\n",
    "# Normalized data\n",
    "##################\n",
    "m = X_train.shape[0] # no of training samples\n",
    "# print(\"No of training samples\", m)\n",
    "n = X_train.shape[1] # no of features\n",
    "# print(\"No of features\", n)\n",
    "X_train_app = np.c_[np.ones(m), X_train_scl] # append a column of 1\n",
    "X_valid_app = np.c_[np.ones(120), X_valid_scl] # append a column of 1\n",
    "X_test_app = np.c_[np.ones(150), X_test_scl] # append a column of 1\n",
    "alpha = 1e-3 # learning rate\n",
    "iters = 1000 # no of iterations\n",
    "# initial_theta = initialize_theta(n)\n",
    "# theta, J = gradient_descent(X=X_train_app, y=y_train, theta=initial_theta, alpha=alpha, iters=iters)\n",
    "# plt.plot(range(0,iters), J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab7def8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T04:02:14.102945Z",
     "iopub.status.busy": "2022-02-12T04:02:14.099841Z",
     "iopub.status.idle": "2022-02-12T04:02:14.158176Z",
     "shell.execute_reply": "2022-02-12T04:02:14.158721Z",
     "shell.execute_reply.started": "2022-02-12T01:38:52.819501Z"
    },
    "papermill": {
     "duration": 0.080698,
     "end_time": "2022-02-12T04:02:14.158895",
     "exception": false,
     "start_time": "2022-02-12T04:02:14.078197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "def flatten_2d(x):\n",
    "    m = x.shape[0] \n",
    "    flat_x = x.view(-1, m) # shape n by m\n",
    "    return flat_x\n",
    "def two_layer_fc(x, params):\n",
    "    \n",
    "    a0 = flatten_2d(x)   # n by m   \n",
    "    w1, b1, w2, b2 = params\n",
    "    \n",
    "    a1 = F.relu(w1.mm(a0) + b1)\n",
    "#     print(\"size of a1\", a1.size())\n",
    "    a2 = torch.sigmoid(w2.mm(a1) + b2)\n",
    "#     print(\"size of a2\", a2.size())\n",
    "    return a2\n",
    "def three_layer_fc(x, params):\n",
    "    \n",
    "    a0 = flatten_2d(x)   # n by m   \n",
    "    w1, b1, w2, b2, w3, b3 = params\n",
    "    \n",
    "    a1 = F.relu(w1.mm(a0) + b1)\n",
    "    a2 = F.relu(w2.mm(a1) + b2)\n",
    "    a3 = torch.sigmoid(w3.mm(a2) + b3)\n",
    "    return a3\n",
    "def four_layer_fc(x, params):\n",
    "    \n",
    "    a0 = flatten_2d(x)   # n by m   \n",
    "    w1, b1, w2, b2, w3, b3, w4, b4 = params\n",
    "    \n",
    "    a1 = F.relu(w1.mm(a0) + b1)\n",
    "    a2 = F.relu(w2.mm(a1) + b2)\n",
    "    a3 = F.relu(w3.mm(a2) + b3)\n",
    "    a4 = torch.sigmoid(w4.mm(a3) + b4)\n",
    "    return a4\n",
    "\n",
    "def two_layer_fc_test():\n",
    "    hidden_layer_size1 = 42\n",
    "    hidden_layer_size2 = 21\n",
    "    hidden_layer_size3 = 10\n",
    "    x = torch.zeros((64, 50), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "    w1 = torch.zeros((hidden_layer_size1, 50), dtype=dtype)\n",
    "    b1 = torch.zeros((hidden_layer_size1, 1), dtype=dtype)\n",
    "    w2 = torch.zeros((hidden_layer_size2, hidden_layer_size1), dtype=dtype)\n",
    "    b2 = torch.zeros((hidden_layer_size2, 1), dtype=dtype)\n",
    "    w3 = torch.zeros((hidden_layer_size3, hidden_layer_size2), dtype=dtype)\n",
    "    b3 = torch.zeros((hidden_layer_size3, 1), dtype=dtype)\n",
    "    w4 = torch.zeros((1, hidden_layer_size3), dtype=dtype)\n",
    "    b4 = torch.zeros((1, 1), dtype=dtype)\n",
    "    scores = four_layer_fc(x, [w1, b1, w2, b2, w3, b3, w4, b4])\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c411183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-12T04:02:14.178603Z",
     "iopub.status.busy": "2022-02-12T04:02:14.177602Z",
     "iopub.status.idle": "2022-02-12T04:02:20.561089Z",
     "shell.execute_reply": "2022-02-12T04:02:20.561635Z",
     "shell.execute_reply.started": "2022-02-12T01:59:05.225604Z"
    },
    "papermill": {
     "duration": 6.395017,
     "end_time": "2022-02-12T04:02:20.561811",
     "exception": false,
     "start_time": "2022-02-12T04:02:14.166794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 47.2639\n",
      "Accuracy tensor(49.1667)\n",
      "Final validation accuracy tensor(0.4917)\n"
     ]
    }
   ],
   "source": [
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "#     if len(shape) == 2:  # FC weight\n",
    "    connections = shape[0]\n",
    "#     else:\n",
    "#         fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    w = torch.randn(shape, dtype=dtype) * np.sqrt(2. / connections)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, dtype=dtype, requires_grad=True)\n",
    "\n",
    "def check_accuracy_part2(x, y, model_fn, params):\n",
    "    \n",
    "#     split = 'val' if loader.dataset.train else 'test'\n",
    "#     print('Checking accuracy on the %s set' % split)\n",
    "#     num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "#         for x, y in loader:\n",
    "#             x = x.to(dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(dtype=torch.int64)\n",
    "#             scores = model_fn(x, params)\n",
    "#             _, preds = scores.max(1)\n",
    "#             num_correct += (preds == y).sum()\n",
    "#             num_samples += preds.size(0)\n",
    "#         acc = float(num_correct) / num_samples     \n",
    "        preds = torch.round(model_fn(x, params))\n",
    "        acc = (preds == torch.transpose(y, 0, 1)).sum() / preds.size(1)\n",
    "        print(\"Accuracy\", 100 * acc)\n",
    "    return preds, acc\n",
    "\n",
    "def train_part2(X_train, y_train, X_valid, y_valid, model_fn, params, learning_rate, iters, print_every=200):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: loss\n",
    "    \"\"\"\n",
    "#     J = np.zeros(iters) #initialize loss\n",
    "#     for t in range(iters):\n",
    "    # Move the data to the proper device (GPU or CPU)\n",
    "    x = X_train.to(dtype=dtype)\n",
    "    y = y_train.to(dtype=dtype)\n",
    "\n",
    "    # Forward pass: compute scores and loss\n",
    "    scores = model_fn(x, params)\n",
    "    loss = F.binary_cross_entropy(scores, torch.transpose(y, 0, 1))\n",
    "#     J[t] = loss\n",
    "    # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "    # graph has requires_grad=True and uses backpropagation to compute the\n",
    "    # gradient of the loss with respect to these Tensors, and stores the\n",
    "    # gradients in the .grad attribute of each Tensor.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters. We don't want to backpropagate through the\n",
    "    # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "    # context manager to prevent a computational graph from being built.\n",
    "    with torch.no_grad():\n",
    "        for w in params:\n",
    "            w -= learning_rate * w.grad\n",
    "\n",
    "            # Manually zero the gradients after running the backward pass\n",
    "            w.grad.zero_()\n",
    "\n",
    "#     if t % print_every == 0:\n",
    "    print('Loss = %.4f' % (loss.item()))\n",
    "    preds, acc = check_accuracy_part2(X_valid, y_valid, model_fn, params)\n",
    "    print(\"Final validation accuracy\", acc)\n",
    "    return preds, acc\n",
    "\n",
    "hidden_layer_size1 = 6145\n",
    "hidden_layer_size2 = 3173\n",
    "hidden_layer_size3 = 1587\n",
    "learning_rate = 1e-3\n",
    "iters = 1000 # no of iterations\n",
    "\n",
    "# w1 = random_weight((hidden_layer_size1, 3 * 64 * 64 + 1))\n",
    "# b1 = zero_weight((hidden_layer_size1, 1))\n",
    "# w2 = random_weight((1, hidden_layer_size1))\n",
    "# b2 = zero_weight((1, 1))\n",
    "# params = [w1, b1, w2, b2]\n",
    "# w1 = random_weight((hidden_layer_size1, 3 * 64 * 64 + 1))\n",
    "# b1 = zero_weight((hidden_layer_size1, 1))\n",
    "# w2 = random_weight((hidden_layer_size2, hidden_layer_size1))\n",
    "# b2 = zero_weight((hidden_layer_size2, 1))\n",
    "# w3 = random_weight((1, hidden_layer_size2))\n",
    "# b3 = zero_weight((1, 1))\n",
    "# params = [w1, b1, w2, b2, w3, b3]\n",
    "w1 = random_weight((hidden_layer_size1, 3 * 64 * 64 + 1))\n",
    "b1 = zero_weight((hidden_layer_size1, 1))\n",
    "w2 = random_weight((hidden_layer_size2, hidden_layer_size1))\n",
    "b2 = zero_weight((hidden_layer_size2, 1))\n",
    "w3 = random_weight((hidden_layer_size3, hidden_layer_size2))\n",
    "b3 = zero_weight((hidden_layer_size3, 1))\n",
    "w4 = random_weight((1, hidden_layer_size3))\n",
    "b4 = zero_weight((1, 1))\n",
    "params = [w1, b1, w2, b2, w3, b3, w4, b4]\n",
    "\n",
    "\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 200\n",
    "\n",
    "# convert numpy array to tensors\n",
    "X_tr_tensor = torch.from_numpy(X_train_app)\n",
    "Y_tr_tensor = torch.from_numpy(y_train)\n",
    "X_val_tensor = (torch.from_numpy(X_valid_app)).to(dtype=dtype)\n",
    "Y_val_tensor = (torch.from_numpy(y_valid)).to(dtype=dtype)\n",
    "\n",
    "preds, acc = train_part2(X_tr_tensor, Y_tr_tensor, X_val_tensor, Y_val_tensor, four_layer_fc, params, learning_rate, iters, print_every=print_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.85698,
   "end_time": "2022-02-12T04:02:21.380963",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-12T04:02:00.523983",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
